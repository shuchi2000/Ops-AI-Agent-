{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcbe4ba7",
   "metadata": {},
   "source": [
    "## Agent 1 - Orchestration - Work to identify the intent and choose the correct file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af0c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_examples = {\n",
    "    \"client_program\": [\n",
    "        \"Which learners are enrolled in client X's program?\",\n",
    "        \"List programs mapped to each learner\",\n",
    "    ],\n",
    "    \"attendance\": [\n",
    "        \"What is the attendance of learner John?\",\n",
    "        \"Show me attendance data for March\",\n",
    "    ],\n",
    "    \"content_progress\": [\n",
    "        \"Which learners have completed module 2?\",\n",
    "        \"How many learners are in progress with the content?\",\n",
    "    ],\n",
    "    \"progress_report\": [\n",
    "        \"Give me the progress status of each learner\",\n",
    "        \"Who hasn't completed the required modules yet?\",\n",
    "    ],\n",
    "    \"score_report\": [\n",
    "        \"What are the scores for learners in the final test?\",\n",
    "        \"Show me the average score per learner\",\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b350fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shuchismita_mallick.Shuchismita\\anaconda3\\envs\\aienv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "def get_intent(query):\n",
    "    labels = list(intent_examples.keys())\n",
    "    result = classifier(query, labels)\n",
    "    return result['labels'][0]  # Most likely intent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99756ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_file(intent):\n",
    "    file_mapping = {\n",
    "        \"client_program\": \"client_program_learner.csv\",\n",
    "        \"attendance\": \"learner_attendence.csv\",\n",
    "        \"content_progress\": \"progress_content_deploy.csv\",\n",
    "        \"progress_report\": \"progress_report.csv\",\n",
    "        \"score_report\": \"score_report.csv\"\n",
    "    }\n",
    "    return f\"..//preparing_data/{file_mapping[intent]}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c3e8a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_query(query):\n",
    "    intent = get_intent(query)\n",
    "    file_path = route_to_file(intent)\n",
    "    return {\n",
    "        \"intent\": intent,\n",
    "        \"file_path\": file_path\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f96f7f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intent': 'client_program', 'file_path': '..//preparing_data/client_program_learner.csv'}\n"
     ]
    }
   ],
   "source": [
    "Client_name = 'Fractal Analytics'\n",
    "Program_code = 'data-science-hackathon-march-25'\n",
    "\n",
    "query = \"How many learners are enrolled?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "    Client: {Client_name}\n",
    "    Program: {Program_code}\n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "\n",
    "print(handle_query(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f6837ee-e619-4df1-85d1-52e27ad24ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "def handle_query(query):\n",
    "    # Load the intent examples from the JSON file\n",
    "    with open(r\"C:\\Users\\shuchismita_mallick.Shuchismita\\GenAI-Projects\\Operation AI Agent\\data\\intent.json\", 'r') as file:\n",
    "        intent_examples = json.load(file)\n",
    "\n",
    "    # Initialize the zero-shot classifier\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "    # Function to get the most likely intent based on the query\n",
    "    def get_intent(query):\n",
    "        labels = list(intent_examples.keys())  # Labels (intents) from the loaded JSON\n",
    "        result = classifier(query, labels)\n",
    "        return result['labels'][0]  # Most likely intent\n",
    "\n",
    "    # Function to map the intent to the corresponding file path\n",
    "    def route_to_file(intent):\n",
    "        file_mapping = {\n",
    "            \"client_program\": \"client_program_learner.csv\",\n",
    "            \"attendance\": \"learner_attendence.csv\",\n",
    "            \"content_progress\": \"progress_content_deploy.csv\",\n",
    "            \"progress_report\": \"progress_report.csv\",\n",
    "            \"score_report\": \"score_report.csv\"\n",
    "        }\n",
    "        \n",
    "        # Base directory\n",
    "        base_dir = r\"C:\\Users\\shuchismita_mallick.Shuchismita\\GenAI-Projects\\Operation AI Agent\\data\"\n",
    "        \n",
    "        # Use os.path.join() to correctly construct the full file path\n",
    "        return os.path.join(base_dir, file_mapping.get(intent, 'default_file.csv'))\n",
    "\n",
    "    # Get the intent from the query\n",
    "    intent = get_intent(query)\n",
    "    \n",
    "    # Get the file path for the identified intent\n",
    "    file_path = route_to_file(intent)\n",
    "    \n",
    "    # Return the intent and corresponding file path\n",
    "    return {\n",
    "        \"intent\": intent,\n",
    "        \"file_path\": file_path\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "843ad368-4b31-4949-98e1-ae35f9f6a1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intent': 'content_progress', 'file_path': 'C:\\\\Users\\\\shuchismita_mallick.Shuchismita\\\\GenAI-Projects\\\\Operation AI Agent\\\\data\\\\progress_content_deploy.csv'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "query = \"What is the status of content progress?\"\n",
    "response = handle_query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9185cdd-e369-4f44-8883-31573f9bde71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "def handle_query(query):\n",
    "    # Load the intent examples from the JSON file\n",
    "    intent_examples = {\n",
    "        \"client_program\": [\n",
    "            \"Which learners are enrolled in client X's program?\",\n",
    "            \"List programs mapped to each learner\",\n",
    "        ],\n",
    "        \"attendance\": [\n",
    "            \"What is the attendance of learner John?\",\n",
    "            \"Show me attendance data for March\",\n",
    "        ],\n",
    "        \"content_progress\": [\n",
    "            \"Which learners have completed module 2?\",\n",
    "            \"How many learners are in progress with the content?\",\n",
    "        ],\n",
    "        \"progress_report\": [\n",
    "            \"Give me the progress status of each learner\",\n",
    "            \"Who hasn't completed the required modules yet?\",\n",
    "        ],\n",
    "        \"score_report\": [\n",
    "            \"What are the scores for learners in the final test?\",\n",
    "            \"Show me the average score per learner\",\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Initialize the zero-shot classifier\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "    # Function to get the most likely intent based on the query\n",
    "    def get_intent(query):\n",
    "        # Flatten all example questions into a list of labels\n",
    "        labels = []\n",
    "        for intent, examples in intent_examples.items():\n",
    "            labels.extend(examples)  # Add all example questions for each intent\n",
    "        \n",
    "        # Perform zero-shot classification\n",
    "        result = classifier(query, labels)\n",
    "\n",
    "        # Match the most likely label to an intent\n",
    "        matched_label = result['labels'][0]\n",
    "        \n",
    "        # Map the matched label back to the intent\n",
    "        for intent, examples in intent_examples.items():\n",
    "            if matched_label in examples:\n",
    "                return intent  # Return the matching intent\n",
    "        \n",
    "        return \"Unknown\"  # If no match is found, return Unknown\n",
    "\n",
    "    # Function to map the intent to the corresponding file path\n",
    "    def route_to_file(intent):\n",
    "        file_mapping = {\n",
    "            \"client_program\": \"client_program_learner.csv\",\n",
    "            \"attendance\": \"learner_attendence.csv\",\n",
    "            \"content_progress\": \"progress_content_deploy.csv\",\n",
    "            \"progress_report\": \"progress_report.csv\",\n",
    "            \"score_report\": \"score_report.csv\"\n",
    "        }\n",
    "\n",
    "        # Base directory\n",
    "        base_dir = r\"C:\\Users\\shuchismita_mallick.Shuchismita\\GenAI-Projects\\Operation AI Agent\\data\"\n",
    "\n",
    "        # Use os.path.join() to correctly construct the full file path\n",
    "        return os.path.join(base_dir, file_mapping.get(intent, 'default_file.csv'))\n",
    "\n",
    "    # Get the intent from the query\n",
    "    intent = get_intent(query)\n",
    "\n",
    "    # Get the file path for the identified intent\n",
    "    file_path = route_to_file(intent)\n",
    "\n",
    "    # Return the intent and corresponding file path\n",
    "    return {\n",
    "        \"intent\": intent,\n",
    "        \"file_path\": file_path\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "be63c655-537b-47ec-90ad-0980f82f055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "def handle_query(query):\n",
    "    # Load the intent examples from the JSON file\n",
    "    intent_examples = {\n",
    "        \"client_program\": [\n",
    "            \"Which learners are enrolled in client X's program?\",\n",
    "            \"List programs mapped to each learner\",\n",
    "        ],\n",
    "        \"attendance\": [\n",
    "            \"What is the attendance of learner John?\",\n",
    "            \"Show me attendance data for March\",\n",
    "        ],\n",
    "        \"content_progress\": [\n",
    "            \"Which learners have completed module 2?\",\n",
    "            \"How many learners are in progress with the content?\",\n",
    "        ],\n",
    "        \"progress_report\": [\n",
    "            \"Give me the progress status of each learner\",\n",
    "            \"Who hasn't completed the required modules yet?\",\n",
    "        ],\n",
    "        \"score_report\": [\n",
    "            \"What are the scores for learners in the final test?\",\n",
    "            \"Show me the average score per learner\",\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Initialize the zero-shot classifier\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "    # Function to get the most likely intent based on the query\n",
    "    def get_intent(query):\n",
    "        # Flatten all example questions into a list of labels\n",
    "        labels = []\n",
    "        for intent, examples in intent_examples.items():\n",
    "            labels.extend(examples)  # Add all example questions for each intent\n",
    "        \n",
    "        # Perform zero-shot classification\n",
    "        result = classifier(query, labels)\n",
    "\n",
    "        # Match the most likely label to an intent\n",
    "        matched_label = result['labels'][0]\n",
    "        \n",
    "        # Map the matched label back to the intent\n",
    "        for intent, examples in intent_examples.items():\n",
    "            if matched_label in examples:\n",
    "                return intent  # Return the matching intent\n",
    "        \n",
    "        return \"Unknown\"  # If no match is found, return Unknown\n",
    "\n",
    "    # Function to map the intent to the corresponding file path\n",
    "    def route_to_file(intent):\n",
    "        file_mapping = {\n",
    "            \"client_program\": \"client_program_learner.csv\",\n",
    "            \"attendance\": \"learner_attendence.csv\",\n",
    "            \"content_progress\": \"progress_content_deploy.csv\",\n",
    "            \"progress_report\": \"progress_report.csv\",\n",
    "            \"score_report\": \"score_report.csv\"\n",
    "        }\n",
    "\n",
    "        # Base directory\n",
    "        base_dir = r\"C:\\Users\\shuchismita_mallick.Shuchismita\\GenAI-Projects\\Operation AI Agent\\data\"\n",
    "\n",
    "        # Use os.path.join() to correctly construct the full file path\n",
    "        return os.path.join(base_dir, file_mapping.get(intent, 'default_file.csv'))\n",
    "\n",
    "    # Get the intent from the query\n",
    "    intent = get_intent(query)\n",
    "\n",
    "    # Get the file path for the identified intent\n",
    "    file_path = route_to_file(intent)\n",
    "\n",
    "    # Return the intent and corresponding file path\n",
    "    return {\n",
    "        \"intent\": intent,\n",
    "        \"file_path\": file_path\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50ea3b87-dfdd-48b0-b56e-333852394a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intent': 'score_report', 'file_path': 'C:\\\\Users\\\\shuchismita_mallick.Shuchismita\\\\GenAI-Projects\\\\Operation AI Agent\\\\data\\\\score_report.csv'}\n"
     ]
    }
   ],
   "source": [
    "Client_name = 'Fractal Analytics'\n",
    "Program_code = 'data-science-hackathon-march-25'\n",
    "\n",
    "query = \"Show me the average score per learner\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "    Client: {Client_name}\n",
    "    Program: {Program_code}\n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "\n",
    "print(handle_query(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1668f215",
   "metadata": {},
   "source": [
    "## Agent 2 - Query Agent - Work to implement the query and provide the raw response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab632f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a621d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from dotenv import load_dotenv\n",
    "from langchain_experimental.agents import create_csv_agent\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "#load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09585fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b812067-1980-40a1-83ae-81344c69bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\shuchismita_mallick.Shuchismita\\GenAI-Projects\\Operation AI Agent\\data\\progress_content_deploy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e70df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(prompt):\n",
    "    # Get the file path based on the intent\n",
    "    file_path = handle_query(prompt)['file_path']\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        # File exists, create the agent\n",
    "        agent = create_csv_agent(llm=llm, path=file_path, verbose=True, allow_dangerous_code=True, handle_parsing_errors=True)\n",
    "        \n",
    "        # Use the agent to answer the query\n",
    "        answer = agent.run(prompt)\n",
    "        \n",
    "        return answer\n",
    "    else:\n",
    "        # File does not exist, return an error message\n",
    "        return \"Sorry, we don't have any information regarding this.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1bad14b0-531b-4269-8f7f-f95bcff79cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Client_name = 'Fractal Analytics'\n",
    "Program_code = 'data-science-hackathon-march-25'\n",
    "\n",
    "query = \"Show me the average score per learner\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "    Client: {Client_name}\n",
    "    Program: {Program_code}\n",
    "    Query: {query}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "00638a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "APIStatusError",
     "evalue": "{'type': 'error', 'error': {'details': None, 'type': 'overloaded_error', 'message': 'Overloaded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIStatusError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:188\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     emit_warning()\n\u001b[1;32m--> 188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain\\chains\\base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    387\u001b[0m }\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain\\chains\\base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain\\chains\\base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    167\u001b[0m     )\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain\\agents\\agent.py:1624\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1622\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1624\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1633\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1634\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain\\agents\\agent.py:1330\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1323\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1327\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1328\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1330\u001b[0m         [\n\u001b[0;32m   1331\u001b[0m             a\n\u001b[0;32m   1332\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[0;32m   1333\u001b[0m                 name_to_tool_map,\n\u001b[0;32m   1334\u001b[0m                 color_mapping,\n\u001b[0;32m   1335\u001b[0m                 inputs,\n\u001b[0;32m   1336\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1337\u001b[0m                 run_manager,\n\u001b[0;32m   1338\u001b[0m             )\n\u001b[0;32m   1339\u001b[0m         ]\n\u001b[0;32m   1340\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain\\agents\\agent.py:1330\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1323\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1327\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1328\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1330\u001b[0m         [\n\u001b[0;32m   1331\u001b[0m             a\n\u001b[0;32m   1332\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[0;32m   1333\u001b[0m                 name_to_tool_map,\n\u001b[0;32m   1334\u001b[0m                 color_mapping,\n\u001b[0;32m   1335\u001b[0m                 inputs,\n\u001b[0;32m   1336\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1337\u001b[0m                 run_manager,\n\u001b[0;32m   1338\u001b[0m             )\n\u001b[0;32m   1339\u001b[0m         ]\n\u001b[0;32m   1340\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain\\agents\\agent.py:1358\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1355\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m-> 1358\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_agent\u001b[38;5;241m.\u001b[39mplan(\n\u001b[0;32m   1359\u001b[0m         intermediate_steps,\n\u001b[0;32m   1360\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1361\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m   1362\u001b[0m     )\n\u001b[0;32m   1363\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain\\agents\\agent.py:465\u001b[0m, in \u001b[0;36mRunnableAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    457\u001b[0m final_output: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_runnable:\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunnable\u001b[38;5;241m.\u001b[39mstream(inputs, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}):\n\u001b[0;32m    466\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    467\u001b[0m             final_output \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3454\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3447\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   3448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstream\u001b[39m(\n\u001b[0;32m   3449\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3452\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3453\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3454\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3440\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3433\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   3434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   3435\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3438\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3439\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3440\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m   3441\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3442\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[0;32m   3443\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   3444\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3445\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain_core\\runnables\\base.py:2218\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   2216\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2217\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 2218\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2219\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m   2220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3402\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   3399\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3400\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n\u001b[1;32m-> 3402\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m final_pipeline\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain_core\\runnables\\base.py:1421\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1418\u001b[0m final: Input\n\u001b[0;32m   1419\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1421\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[0;32m   1422\u001b[0m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[0;32m   1425\u001b[0m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[0;32m   1430\u001b[0m         final \u001b[38;5;241m=\u001b[39m ichunk\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain_core\\runnables\\base.py:5683\u001b[0m, in \u001b[0;36mRunnableBindingBase.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5676\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   5677\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   5678\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5681\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   5682\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 5683\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   5684\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5686\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5687\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain_core\\runnables\\base.py:1439\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1436\u001b[0m             final \u001b[38;5;241m=\u001b[39m ichunk\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[1;32m-> 1439\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:434\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrate_limiter\u001b[38;5;241m.\u001b[39macquire(blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 434\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    436\u001b[0m             chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_manager\u001b[38;5;241m.\u001b[39mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain_anthropic\\chat_models.py:1001\u001b[0m, in \u001b[0;36mChatAnthropic._stream\u001b[1;34m(self, messages, stop, run_manager, stream_usage, **kwargs)\u001b[0m\n\u001b[0;32m    995\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[0;32m    996\u001b[0m coerce_content_to_string \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m _tools_in_params(payload)\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _documents_in_params(payload)\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _thinking_in_params(payload)\n\u001b[0;32m   1000\u001b[0m )\n\u001b[1;32m-> 1001\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[0;32m   1002\u001b[0m     msg \u001b[38;5;241m=\u001b[39m _make_message_chunk_from_anthropic_event(\n\u001b[0;32m   1003\u001b[0m         event,\n\u001b[0;32m   1004\u001b[0m         stream_usage\u001b[38;5;241m=\u001b[39mstream_usage,\n\u001b[0;32m   1005\u001b[0m         coerce_content_to_string\u001b[38;5;241m=\u001b[39mcoerce_content_to_string,\n\u001b[0;32m   1006\u001b[0m     )\n\u001b[0;32m   1007\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\anthropic\\_streaming.py:68\u001b[0m, in \u001b[0;36mStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[_T]:\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator:\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aienv\\lib\\site-packages\\anthropic\\_streaming.py:110\u001b[0m, in \u001b[0;36mStream.__stream__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m             err_msg \u001b[38;5;241m=\u001b[39m sse\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 110\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_make_status_error(\n\u001b[0;32m    111\u001b[0m             err_msg,\n\u001b[0;32m    112\u001b[0m             body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    113\u001b[0m             response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse,\n\u001b[0;32m    114\u001b[0m         )\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Ensure the entire stream is consumed\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _sse \u001b[38;5;129;01min\u001b[39;00m iterator:\n",
      "\u001b[1;31mAPIStatusError\u001b[0m: {'type': 'error', 'error': {'details': None, 'type': 'overloaded_error', 'message': 'Overloaded'}}"
     ]
    }
   ],
   "source": [
    "agent(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "41dad52a-a893-4d5b-b8d8-aa4a1dd66069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'intent': 'client_program',\n",
       " 'file_path': 'C:\\\\Users\\\\shuchismita_mallick.Shuchismita\\\\GenAI-Projects\\\\Operation AI Agent\\\\data\\\\client_program_learner.csv'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_query(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a569a19-a00f-42fa-b0ac-f7cab6d4ff6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shuchismita_mallick.Shuchismita\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain_experimental\\agents\\agent_toolkits\\pandas\\base.py:283: UserWarning: Received additional kwargs {'handle_parsing_errors': True} which are no longer supported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI notice I need to:\n",
      "1. Filter the data for Fractal Analytics client\n",
      "2. Filter for specific program (looking for program code containing \"data-science-hackathon-march-25\")\n",
      "3. Count how many learners have status \"SUBMITTED\"\n",
      "\n",
      "Let me execute these steps:\n",
      "\n",
      "Thought: First, let's check if we have any entries for this specific program for Fractal Analytics\n",
      "\n",
      "Action: python_repl_ast\n",
      "Action Input: df[(df['Client_name'] == 'Fractal Analytics') & (df['code'].str.contains('data-science-hackathon-march-25', na=False))].shape[0]\n",
      "\u001b[32;1m\u001b[1;3mLet me help you complete this analysis. I can see there are entries for Fractal Analytics, but let's count specifically how many learners have \"SUBMITTED\" status for this specific program.\n",
      "\n",
      "Action: python_repl_ast\n",
      "Action Input: df[(df['Client_name'] == 'Fractal Analytics') & \n",
      "   (df['code'].str.contains('data-science-hackathon-march-25', na=False)) & \n",
      "   (df['status'] == 'SUBMITTED')]['learner_id'].nunique()\n",
      "\u001b[32;1m\u001b[1;3mLet me provide the final answer based on the analysis:\n",
      "\n",
      "Final Answer: For Fractal Analytics' data-science-hackathon-march-25 program, 35 learners have submitted their content.\n",
      "\n",
      "This was determined by:\n",
      "1. Filtering for Fractal Analytics clients\n",
      "2. Filtering for the specific program code containing \"data-science-hackathon-march-25\"\n",
      "3. Counting unique learners (using learner_id) who have a \"SUBMITTED\" status\n",
      "4. The result shows 35 unique learners have submitted their content in this program.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'For Fractal Analytics\\' data-science-hackathon-march-25 program, 35 learners have submitted their content.\\n\\nThis was determined by:\\n1. Filtering for Fractal Analytics clients\\n2. Filtering for the specific program code containing \"data-science-hackathon-march-25\"\\n3. Counting unique learners (using learner_id) who have a \"SUBMITTED\" status\\n4. The result shows 35 unique learners have submitted their content in this program.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file_path = r'C:\\Users\\shuchismita_mallick.Shuchismita\\GenAI-Projects\\Operation AI Agent\\data\\progress_report.csv'\n",
    "agent = create_csv_agent(llm=llm, path=file_path, verbose=True, allow_dangerous_code=True, handle_parsing_errors=True)\n",
    "answer = agent.run(prompt)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff844d-54c5-4fad-9c3b-589e706d2ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fa31989-db92-4ee7-8f7b-fedf98462162",
   "metadata": {},
   "source": [
    "## Testing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff8854e7-5464-4459-af13-d5ad273a1d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "def handle_query(query):\n",
    "    # Load the intent examples from the JSON file\n",
    "    intent_examples = {\n",
    "        \"client_program\": [\n",
    "            \"Which learners are enrolled in client X's program?\",\n",
    "            \"List programs mapped to each learner\",\n",
    "        ],\n",
    "        \"attendance\": [\n",
    "            \"What is the attendance of learner John?\",\n",
    "            \"Show me attendance data for March\",\n",
    "        ],\n",
    "        \"content_progress\": [\n",
    "            \"Which learners have completed module 2?\",\n",
    "            \"How many learners are in progress with the content?\",\n",
    "        ],\n",
    "        \"progress_report\": [\n",
    "            \"Give me the progress status of each learner\",\n",
    "            \"Who hasn't completed the required modules yet?\",\n",
    "        ],\n",
    "        \"score_report\": [\n",
    "            \"What are the scores for learners in the final test?\",\n",
    "            \"Show me the average score per learner\",\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Initialize the zero-shot classifier\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "    # Function to get the most likely intent based on the query\n",
    "    def get_intent(query):\n",
    "        # Flatten all example questions into a list of labels\n",
    "        labels = []\n",
    "        for intent, examples in intent_examples.items():\n",
    "            labels.extend(examples)  # Add all example questions for each intent\n",
    "        \n",
    "        # Perform zero-shot classification\n",
    "        result = classifier(query, labels)\n",
    "\n",
    "        # Match the most likely label to an intent\n",
    "        matched_label = result['labels'][0]\n",
    "        \n",
    "        # Map the matched label back to the intent\n",
    "        for intent, examples in intent_examples.items():\n",
    "            if matched_label in examples:\n",
    "                return intent  # Return the matching intent\n",
    "        \n",
    "        return \"Unknown\"  # If no match is found, return Unknown\n",
    "\n",
    "    # Function to map the intent to the corresponding file path\n",
    "    def route_to_file(intent):\n",
    "        file_mapping = {\n",
    "            \"client_program\": \"client_program_learner.csv\",\n",
    "            \"attendance\": \"learner_attendence.csv\",\n",
    "            \"content_progress\": \"progress_content_deploy.csv\",\n",
    "            \"progress_report\": \"progress_report.csv\",\n",
    "            \"score_report\": \"score_report.csv\"\n",
    "        }\n",
    "\n",
    "        # Base directory\n",
    "        base_dir = r\"C:\\Users\\shuchismita_mallick.Shuchismita\\GenAI-Projects\\Operation AI Agent\\data\"\n",
    "\n",
    "        # Use os.path.join() to correctly construct the full file path\n",
    "        return os.path.join(base_dir, file_mapping.get(intent, 'default_file.csv'))\n",
    "\n",
    "    # Get the intent from the query\n",
    "    intent = get_intent(query)\n",
    "\n",
    "    # Get the file path for the identified intent\n",
    "    file_path = route_to_file(intent)\n",
    "\n",
    "    # Return the intent and corresponding file path\n",
    "    return {\n",
    "        \"intent\": intent,\n",
    "        \"file_path\": file_path\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c07575e-6626-485d-a569-cbdbc8a6258b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intent': 'score_report', 'file_path': 'C:\\\\Users\\\\shuchismita_mallick.Shuchismita\\\\GenAI-Projects\\\\Operation AI Agent\\\\data\\\\score_report.csv'}\n"
     ]
    }
   ],
   "source": [
    "Client_name = 'Fractal Analytics'\n",
    "Program_code = 'data-science-hackathon-march-25'\n",
    "\n",
    "query = \"Show me the average score per learner\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "    Client: {Client_name}\n",
    "    Program: {Program_code}\n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "\n",
    "print(handle_query(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d2ac5e-cb4a-4844-b471-86dec4ac24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_experimental.agents import create_csv_agent\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "\n",
    "# Initialize the LLM (Claude 3)\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n",
    "\n",
    "def agent(prompt):\n",
    "    \"\"\"\n",
    "    This function handles the incoming prompt, fetches the corresponding file path based on intent,\n",
    "    and returns the answer by utilizing the agent.\n",
    "    \"\"\"\n",
    "    # Get the file path based on the intent extracted from the query\n",
    "    file_path = handle_query(prompt)['file_path']\n",
    "    \n",
    "    # Check if the file exists at the determined path\n",
    "    if os.path.exists(file_path):\n",
    "        # File exists, create the CSV agent\n",
    "        agent = create_csv_agent(\n",
    "            llm=llm, \n",
    "            path=file_path, \n",
    "            verbose=True, \n",
    "            allow_dangerous_code=True, \n",
    "            handle_parsing_errors=True\n",
    "        )\n",
    "        \n",
    "        # Use the agent to answer the query\n",
    "        answer = agent.run(prompt)\n",
    "        \n",
    "        return answer\n",
    "    else:\n",
    "        # File does not exist, return an error message\n",
    "        return \"Sorry, we don't have any information regarding this.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aa01f6b7-ffe9-4ffb-b6d6-6c0dd10ad168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "C:\\Users\\shuchismita_mallick.Shuchismita\\anaconda3\\envs\\aienv\\lib\\site-packages\\langchain_experimental\\agents\\agent_toolkits\\pandas\\base.py:283: UserWarning: Received additional kwargs {'handle_parsing_errors': True} which are no longer supported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLet me help you analyze the average scores for learners in the data science hackathon program for Fractal Analytics.\n",
      "\n",
      "Thought: I need to:\n",
      "1. Filter the dataframe for Fractal Analytics and data-science-hackathon-march-25\n",
      "2. Calculate the average of total_overall_score for each learner_id\n",
      "\n",
      "Action: python_repl_ast\n",
      "Action Input: \n",
      "df_filtered = df[\n",
      "    (df['Client_name'] == 'Fractal Analytics') & \n",
      "    (df['code'] == 'data-science-hackathon-march-25')\n",
      "]\n",
      "avg_scores = df_filtered.groupby('learner_name')['total_overall_score'].mean().reset_index()\n",
      "print(avg_scores)\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m                     learner_name  total_overall_score\n",
      "0                Aaradhya  Wadhwa                 80.0\n",
      "1                Abhilash  Biswas                 80.0\n",
      "2              Abira  Chakrabarty                 80.0\n",
      "3                Aditya  Lahariya                 80.0\n",
      "4   Adityanarayanan  Madhusudanan                 80.0\n",
      "..                            ...                  ...\n",
      "91            Yashaswi  Bhatnagar                 80.0\n",
      "92             Yashwant  Bhaidkar                 80.0\n",
      "93                Yogeeta  Shetty                 80.0\n",
      "94                   mayank kumar                 80.0\n",
      "95               mohan krishna kc                 80.0\n",
      "\n",
      "[96 rows x 2 columns]\n",
      "\u001b[32;1m\u001b[1;3mLet me analyze this data:\n",
      "\n",
      "1. I notice that for this particular program (data-science-hackathon-march-25) at Fractal Analytics:\n",
      "- All learners have received the same score of 80.0\n",
      "- There are 96 total learners who participated\n",
      "- The scores appear to be uniform across all participants\n",
      "\n",
      "This could indicate that this might have been a participation score or an initial scoring, since everyone received the same score.\n",
      "\n",
      "Final Answer: For the data science hackathon program at Fractal Analytics, all 96 learners received a score of 80.0, making the average score 80.0 across all participants. This uniform scoring suggests it might be a participation score or initial assessment score given to all participants.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'For the data science hackathon program at Fractal Analytics, all 96 learners received a score of 80.0, making the average score 80.0 across all participants. This uniform scoring suggests it might be a participation score or initial assessment score given to all participants.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e385048-13d1-40c4-82ea-df2ab578fdeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
